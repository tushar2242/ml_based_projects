{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4b7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb58a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Node.js is a cross-platform, open-source server environment that can run on Windows, Linux, Unix, macOS, and more. Node.js is a back-end JavaScript runtime environment, runs on the V8 JavaScript engine, and executes JavaScript code outside a web browser.\n",
    "\n",
    "Node.js lets developers use JavaScript to write command line tools and for server-side scripting. The ability to run JavaScript code on the server is often used to generate dynamic web page content before the page is sent to the user's web browser. Consequently, Node.js represents a \"JavaScript everywhere\" paradigm,[5] unifying web-application development around a single programming language, as opposed to using different languages for the server- versus client-side programming.\n",
    "\n",
    "Node.js has an event-driven architecture capable of asynchronous I/O. These design choices aim to optimize throughput and scalability in web applications with many input/output operations, as well as for real-time Web applications (e.g., real-time communication programs and browser games).[6]\n",
    "\n",
    "The Node.js distributed development project was previously governed by the Node.js Foundation,[7] and has now merged with the JS Foundation to form the OpenJS Foundation. OpenJS Foundation is facilitated by the Linux Foundation's Collaborative Projects program.[8]\n",
    "\n",
    "History\n",
    "\n",
    "Ryan Dahl, creator of Node.js, in 2010\n",
    "Node.js was written initially by Ryan Dahl in 2009,[9] about thirteen years after the introduction of the first server-side JavaScript environment, Netscape's LiveWire Pro Web.[10] The initial release supported only Linux and Mac OS X. Its development and maintenance was led by Dahl and later sponsored by Joyent.[11]\n",
    "\n",
    "Dahl criticized the limited possibilities of the most popular web server in 2009, Apache HTTP Server, to handle a lot of concurrent connections (up to 10,000 and more) and the most common way of creating code (sequential programming), when code either blocked the entire process or implied multiple execution stacks in the case of simultaneous connections.[12]\n",
    "\n",
    "Dahl demonstrated the project at the inaugural European JSConf on November 8, 2009.[13][14][15] Node.js combined Google's V8 JavaScript engine, an event loop, and a low-level I/O API.[16]\n",
    "\n",
    "In January 2010, a package manager was introduced for the Node.js environment called npm.[17] The package manager makes it easier for programmers to publish and share source code of Node.js packages and is designed to simplify installation, updating, and uninstallation of packages.[16]\n",
    "\n",
    "In June 2011, Microsoft and Joyent implemented a native Windows version of Node.js.[18] The first Node.js build supporting Windows was released in July 2011.\n",
    "\n",
    "In January 2012, Dahl stepped aside, promoting coworker and npm creator Isaac Schlueter to manage the project.[19] In January 2014, Schlueter announced that Timothy J. Fontaine would lead the project.[20]\n",
    "\n",
    "In December 2014, Fedor Indutny started io.js, a fork of Node.js. Due to the internal conflict over Joyent's governance, io.js was created as an open governance alternative with a separate technical committee.[21][22] Unlike Node.js,[23] the authors planned to keep io.js up-to-date with the latest releases of the Google V8 JavaScript engine.[24]\n",
    "\n",
    "In February 2015, the intent to form a neutral Node.js Foundation was announced. By June 2015, the Node.js and io.js communities voted to work together under the Node.js Foundation.[25]\n",
    "\n",
    "In September 2015, Node.js v0.12 and io.js v3.3 were merged back together into Node v4.0.[26] This merge brought V8 ES6 features into Node.js and a long-term support release cycle.[27] As of 2016, the io.js website recommends that developers switch back to Node.js and that no further releases of io.js are planned due to the merge.[28]\n",
    "\n",
    "In 2019, the JS Foundation and Node.js Foundation merged to form the OpenJS Foundation.\n",
    "\n",
    "On March 15 2023, Node.js 19.8.1 was released.[29]\n",
    "\n",
    "Overview\n",
    "Node.js allows the creation of Web servers and networking tools using JavaScript and a collection of \"modules\" that handle various core functionalities.[13][16][30][31][32] Modules are provided for file system I/O, networking (DNS, HTTP, TCP, TLS/SSL, or UDP), binary data (buffers), cryptography functions, data streams, and other core functions.[16][31][33] Node.js's modules use an API designed to reduce the complexity of writing server applications.[16][31]\n",
    "\n",
    "JavaScript is the only language that Node.js supports natively, but many compile-to-JS languages are available.[34] As a result, Node.js applications can be written in CoffeeScript,[35] Dart, TypeScript, ClojureScript and others.\n",
    "\n",
    "Node.js is primarily used to build network programs such as Web servers.[30] The most significant difference between Node.js and PHP is that most functions in PHP block until completion (commands execute only after previous commands finish), while Node.js functions are non-blocking (commands execute concurrently or even in parallel,[36][37] and use callbacks to signal completion or failure).[30]\n",
    "\n",
    "Node.js is officially supported on Linux, macOS and Microsoft Windows 8.1 and Server 2012 (and later),[2] with tier 2 support for SmartOS and IBM AIX and experimental support for FreeBSD. OpenBSD also works, and LTS versions available for IBM i (AS/400).[38] The provided source code may also be built on similar operating systems to those officially supported or be modified by third parties to support others such as NonStop OS[39] and Unix servers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ff916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text = text.replace('\\n\\n','')\n",
    "# cleaned_text = cleaned_text.replace('\\ ','')\n",
    "# cleaned_text = cleaned_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a7c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead8cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b11b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2a4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9582338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237d929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and configure the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.85,  # Exclude terms appearing in more than 85% of documents\n",
    "    min_df=2,     # Exclude terms appearing in fewer than 2 documents\n",
    "    stop_words=None,  # Do not remove stop words\n",
    "    lowercase=True,   # Convert text to lowercase\n",
    ")\n",
    "\n",
    "# vectorizer.fit(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5975d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit and transform your text data to obtain TF-IDF vectors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tfidf_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1397\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1395\u001b[0m min_doc_count \u001b[38;5;241m=\u001b[39m min_df \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(min_df, Integral) \u001b[38;5;28;01melse\u001b[39;00m min_df \u001b[38;5;241m*\u001b[39m n_doc\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_doc_count \u001b[38;5;241m<\u001b[39m min_doc_count:\n\u001b[1;32m-> 1397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_df corresponds to < documents than min_df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1399\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "\u001b[1;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit and transform your text data to obtain TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectors = vectorizer.transform(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee53787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq = {}\n",
    "\n",
    "# for word in doc:\n",
    "#     if word.text.lower() not in stop_words and word.text.lower() not in punctuation:\n",
    "#         if word.text not in word_freq.keys():\n",
    "#             word_freq[word.text]=1\n",
    "#         else:\n",
    "#             word_freq[word.text]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff47d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_freq = max(word_freq.values())\n",
    "# max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization the value of the given text frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c930ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in word_freq.keys():\n",
    "#     word_freq[word] = word_freq[word]/max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d251e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq\n",
    "# #normalize the value `1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token = [sent for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc88466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores = {}\n",
    "\n",
    "for i, sent in enumerate(sent_token):\n",
    "    score = 0\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    print(f\"Sentence {i}: {sent}\")  # Print the sentence for debugging\n",
    "    for word in sent:\n",
    "        if word.text in vocab:\n",
    "            word_index = vocab[word.text]\n",
    "            print(f\"Word: {word.text}, Word Index: {word_index}, TF-IDF Value: {tfidf_vectors[i, word_index]}\")\n",
    "            score += tfidf_vectors[i, word_index]\n",
    "    sent_scores[sent] = score\n",
    "\n",
    "# Now, sent_scores will contain scores for each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sent_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_len = int(len(sent_score)*0.11)\n",
    "select_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63802947",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = nlargest(select_len,sent_score,key=sent_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa915e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_text = ''\n",
    "for token in summary:\n",
    "    summary_text +=(token.text)\n",
    "\n",
    "summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a147891",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeb55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21695495",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
